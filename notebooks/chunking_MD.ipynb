{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ecd5a3e",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace89717",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13618f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import markdown      \n",
    "from bs4 import BeautifulSoup  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93e5748",
   "metadata": {},
   "source": [
    "### Defining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "801d5e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_header(md_text: str, header_level: int = 1):\n",
    "    \"\"\"\n",
    "    Split markdown text by a given header level (#, ##, ###, etc.)\n",
    "    Returns a list of tuples (title, section_text)\n",
    "    \"\"\"\n",
    "    header_pattern = {\n",
    "        1: r'^\\#\\s+',\n",
    "        2: r'^\\#\\#\\s+',\n",
    "        3: r'^\\#\\#\\#\\s+',\n",
    "    }.get(header_level, r'^\\#\\s+')\n",
    "\n",
    "    headers = []\n",
    "    for m in re.finditer(rf'{header_pattern}(.+)', md_text, flags=re.MULTILINE):\n",
    "        headers.append((m.start(), m.group(1).strip()))\n",
    "\n",
    "    chunks = []\n",
    "    if not headers:\n",
    "        chunks.append((\"\", md_text.strip()))\n",
    "        return chunks\n",
    "\n",
    "    for i, (start, title) in enumerate(headers):\n",
    "        end = headers[i + 1][0] if i + 1 < len(headers) else len(md_text)\n",
    "        section = md_text[start:end].strip()\n",
    "        chunks.append((title, section))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def has_html_table(html_text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if the given HTML text contains at least one <table> element.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    return bool(soup.find(\"table\"))\n",
    "\n",
    "\n",
    "def extract_context_sentences(text: str, num_sentences: int = 3) -> str:\n",
    "    \"\"\"\n",
    "    Extract the last N sentences from text to use as context.\n",
    "    \n",
    "    Args:\n",
    "        text: Text to extract sentences from\n",
    "        num_sentences: Number of sentences to extract (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "        String containing the last N sentences\n",
    "    \"\"\"\n",
    "    # Simple sentence splitting (handles ., !, ?)\n",
    "    sentence_pattern = r'[.!?]+\\s+'\n",
    "    sentences = re.split(sentence_pattern, text.strip())\n",
    "    \n",
    "    # Filter out empty sentences\n",
    "    sentences = [s.strip() for s in sentences if s.strip()]\n",
    "    \n",
    "    # Get last N sentences\n",
    "    context_sentences = sentences[-num_sentences:] if len(sentences) >= num_sentences else sentences\n",
    "    \n",
    "    # Rejoin with proper punctuation\n",
    "    if context_sentences:\n",
    "        return '. '.join(context_sentences) + '.'\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def md_chunk_to_fields(title: str, chunk_md: str, context_text: str = \"\"):\n",
    "    \"\"\"\n",
    "    Convert one markdown section to structured fields.\n",
    "    \n",
    "    Args:\n",
    "        title: Section title\n",
    "        chunk_md: Markdown content\n",
    "        context_text: Optional context text to prepend (for table chunks)\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with structured fields:\n",
    "        - raw markdown\n",
    "        - HTML\n",
    "        - plain text\n",
    "        - tables (as HTML and text)\n",
    "        - concatenated_text (for embedding)\n",
    "        - is_table: flag indicating if chunk contains a table\n",
    "        - has_context: flag indicating if context was prepended\n",
    "    \"\"\"\n",
    "    # Detect if this chunk has a table\n",
    "    is_table = has_table(chunk_md)\n",
    "    \n",
    "    # Convert markdown to HTML\n",
    "    html = markdown.markdown(chunk_md, extensions=['tables', 'fenced_code'])\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Extract tables\n",
    "    tables_html = []\n",
    "    tables_text_parts = []\n",
    "    for tbl in soup.find_all('table'):\n",
    "        tables_html.append(str(tbl))\n",
    "        rows = []\n",
    "        for tr in tbl.find_all('tr'):\n",
    "            cells = [c.get_text(strip=True) for c in tr.find_all(['th', 'td'])]\n",
    "            rows.append(\" | \".join(cells))\n",
    "        tables_text_parts.append(\"\\n\".join(rows))\n",
    "\n",
    "    # Remove tables from main HTML for clean text\n",
    "    for t in soup.find_all('table'):\n",
    "        t.decompose()\n",
    "    plain_text = soup.get_text(separator=\"\\n\").strip()\n",
    "\n",
    "    # Build concatenated text for embedding\n",
    "    # If this is a table chunk and we have context, prepend it\n",
    "    if is_table and context_text:\n",
    "        # Prepend context to help embeddings understand table meaning\n",
    "        concatenated_text = f\"Context: {context_text}\\n\\n{title}\\n\\n{plain_text}\\n\\n\" + \"\\n\\n\".join(tables_text_parts)\n",
    "        has_context = True\n",
    "    else:\n",
    "        concatenated_text = f\"{title}\\n\\n{plain_text}\\n\\n\" + \"\\n\\n\".join(tables_text_parts)\n",
    "        has_context = False\n",
    "\n",
    "    return {\n",
    "        \"title\": title,\n",
    "        \"raw_markdown\": chunk_md,\n",
    "        \"html\": html,\n",
    "        \"text\": plain_text,\n",
    "        \"tables_html\": tables_html,\n",
    "        \"tables_text\": tables_text_parts,\n",
    "        \"concatenated_text\": concatenated_text.strip(),\n",
    "        \"is_table\": is_table,\n",
    "        \"has_context\": has_context,\n",
    "        \"context\": context_text if has_context else \"\"\n",
    "    }\n",
    "\n",
    "\n",
    "def process_chunks_with_context(sections: list, num_context_sentences: int = 3) -> list:\n",
    "    \"\"\"\n",
    "    Process all chunks and add context to table chunks from previous chunks.\n",
    "    \n",
    "    Args:\n",
    "        sections: List of (title, content) tuples\n",
    "        num_context_sentences: Number of sentences to extract as context\n",
    "    \n",
    "    Returns:\n",
    "        List of processed chunk dictionaries\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    for i, (title, content) in enumerate(sections):\n",
    "        # Check if this chunk contains a table\n",
    "        if has_table(content):\n",
    "            # Extract context from previous chunks (up to 3 chunks back)\n",
    "            context_parts = []\n",
    "            for j in range(max(0, i - 3), i):\n",
    "                prev_title, prev_content = sections[j]\n",
    "                # Extract text content (without the markdown table markup)\n",
    "                prev_html = markdown.markdown(prev_content, extensions=['tables', 'fenced_code'])\n",
    "                prev_soup = BeautifulSoup(prev_html, 'html.parser')\n",
    "                # Remove tables from previous chunk\n",
    "                for tbl in prev_soup.find_all('table'):\n",
    "                    tbl.decompose()\n",
    "                prev_text = prev_soup.get_text(separator=\" \").strip()\n",
    "                \n",
    "                if prev_text:\n",
    "                    # Extract last few sentences from this previous chunk\n",
    "                    sentences = extract_context_sentences(prev_text, num_context_sentences)\n",
    "                    if sentences:\n",
    "                        context_parts.append(sentences)\n",
    "            \n",
    "            # Use the most recent context (last chunk with text)\n",
    "            context = context_parts[-1] if context_parts else \"\"\n",
    "        else:\n",
    "            context = \"\"\n",
    "        \n",
    "        # Process the chunk with context\n",
    "        record = md_chunk_to_fields(title, content, context)\n",
    "        record[\"id\"] = i + 1\n",
    "        data.append(record)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd08de",
   "metadata": {},
   "source": [
    "### Loading markdown file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaed0128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Educating to Lead\n",
      "\n",
      "Ministry of Higher Education and Scientific Research University of Tunis\n",
      "\n",
      "Tunis Business School\n",
      "\n",
      "‚ÄúEducating Future Leaders and Managers for a Global Economy‚Äù\n",
      "\n",
      "SCHOOL HANDBOOK\n",
      "\n",
      "Version: September, 2022\n",
      "\n",
      "Last update: February 5, 2023\n",
      "\n",
      "# DISCLAIMER\n",
      "\n",
      "This Handbook provides information about the school, its programs, guidelines, and regulations. It has been approved by the Scientific Council. It is the only body in the school that can formally modify this handbook.\n",
      "\n",
      "Tunis Business \n"
     ]
    }
   ],
   "source": [
    "input_file = Path(\"/home/codepips/Home/Portfolio/Projects/ŸÖÿ≥ÿßÿ±/data/processed/MD/TBS_Handbook-2022.md\")  \n",
    "md_text = input_file.read_text(encoding=\"utf-8\")\n",
    "print(md_text[:500])  # preview first 500 chars\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91d52b1",
   "metadata": {},
   "source": [
    "### Splitting markdown by \\#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6d689841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 88 sections\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'DISCLAIMER'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sections = split_by_header(md_text, header_level=1)\n",
    "print(f\"Found {len(sections)} sections\")\n",
    "\n",
    "# Preview first section title\n",
    "sections[0][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad70f1",
   "metadata": {},
   "source": [
    "### Converting chunks into JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4a6a2fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 88 chunks\n",
      "\n",
      "üìä Statistics:\n",
      "   Total chunks: 88\n",
      "   Table chunks: 0\n",
      "   Chunks with context: 0\n"
     ]
    }
   ],
   "source": [
    "# Process chunks with context for tables\n",
    "data = process_chunks_with_context(sections, num_context_sentences=3)\n",
    "\n",
    "print(f\"Processed {len(data)} chunks\")\n",
    "\n",
    "# Show statistics\n",
    "table_chunks = [d for d in data if d['is_table']]\n",
    "chunks_with_context = [d for d in data if d['has_context']]\n",
    "\n",
    "print(f\"\\nüìä Statistics:\")\n",
    "print(f\"   Total chunks: {len(data)}\")\n",
    "print(f\"   Table chunks: {len(table_chunks)}\")\n",
    "print(f\"   Chunks with context: {len(chunks_with_context)}\")\n",
    "\n",
    "# Show example of a table chunk with context\n",
    "if chunks_with_context:\n",
    "    print(f\"\\nüìã Example table chunk with context:\")\n",
    "    example = chunks_with_context[0]\n",
    "    print(f\"   ID: {example['id']}\")\n",
    "    print(f\"   Title: {example['title']}\")\n",
    "    print(f\"   Context: {example['context'][:150]}...\")\n",
    "    print(f\"   Has table: {example['is_table']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56ddd134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"title\": \"DISCLAIMER\",\n",
      "  \"raw_markdown\": \"# DISCLAIMER\\n\\nThis Handbook provides information about the school, its programs, guidelines, and regulations. It has been approved by the Scientific Council. It is the only body in the school that can formally modify this handbook.\\n\\nTunis Business School reserves the right to amend any policy at any time. The most updated version is the online version (updated on 5 February 2023). It is the responsibility of the students to be familiar with the content of this handbook.\",\n",
      "  \"html\": \"<h1>DISCLAIMER</h1>\\n<p>This Handbook provides information about the school, its programs, guidelines, and regulations. It has been approved by the Scientific Council. It is the only body in the school that can formally modify this handbook.</p>\\n<p>Tunis Busine\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(data[0], indent=2)[:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "174999e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 88 chunks to chunks.jsonl\n"
     ]
    }
   ],
   "source": [
    "output_file = Path(\"chunks.jsonl\")\n",
    "with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    for record in data:\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Saved {len(data)} chunks to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
